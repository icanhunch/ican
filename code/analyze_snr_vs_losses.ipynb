{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import stats\n",
    "from numpy import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant values\n",
    "pwd_path = os.getcwd()\n",
    "output_path = pwd_path + '/../data/weighted-datasets'\n",
    "output_path = pwd_path + '/../data/non-weighted-datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_name_to_index(dataset_name):\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A note regarding stats_norm_interval_division_factor_const:\n",
    "# The stats_norm_interval_division_factor_const parameter is set to select the MPIW width / PICP\n",
    "# working point abd is set differently in different datasets - This is a hyper-parameter of the model selected\n",
    "# by the user.\n",
    "\n",
    "#dataset_name = 'A'\n",
    "#stats_norm_interval_division_factor_const = 5\n",
    "\n",
    "#dataset_name = 'B'\n",
    "#stats_norm_interval_division_factor_const = 10\n",
    "\n",
    "#dataset_name = 'C'\n",
    "#stats_norm_interval_division_factor_const = 10\n",
    "\n",
    "dataset_name = 'D'\n",
    "stats_norm_interval_division_factor_const = 10\n",
    "\n",
    "#dataset_name = 'E'\n",
    "#stats_norm_interval_division_factor_const = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = output_path + '/snr_predictions_vs_losses_' + dataset_name + '.csv'\n",
    "df = pd.read_csv(csv_name) \n",
    "\n",
    "# Save all output vectors to this CSV\n",
    "csv_vectors_output_filename = output_path + '/snr_predictions_results_dataset_' +\\\n",
    "                              dataset_name + '.csv'\n",
    "df_snr_results = pd.DataFrame()\n",
    "\n",
    "csv_name_validation = output_path + '/snr_predictions_vs_losses_validation_' + dataset_name + '.csv'\n",
    "df_validation = pd.read_csv(csv_name_validation)\n",
    "\n",
    "infinity_loss_definition = 10e16\n",
    "\n",
    "# Fix some bug (these are the validation losses, irrelevant for what we are trying to achieve)\n",
    "if ('losses_val' not in df.columns):\n",
    "    df['losses_val'] = 1\n",
    "\n",
    "df = df[df['losses_val'] < infinity_loss_definition]\n",
    "df = df[df['losses_val'] > -infinity_loss_definition]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#ax.plot(SNR_predictions, losses,label=\"Prediction losses [%] vs. SNR per prediction\")\n",
    "\n",
    "ax.scatter(df['SNR_predictions'], df['losses_val'])\n",
    "#ax.title('Prediction losses [%] vs. SNR per prediction')\n",
    "plt.gca().update(dict(title='Prediction losses [%] vs. SNR per prediction [Low SNR]'))\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(\"SNR\")\n",
    "ax.set_ylabel(\"loss %\")\n",
    "\n",
    "print('Variance of losses =', df['losses_val'].var(), df['losses_val'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity1 = 0.5 * (math.log2(1 + 80))\n",
    "capacity2 = 0.5 * (math.log2(1 + 4000))\n",
    "capacity3 = 0.5 * (math.log2(1 + 10000))\n",
    "\n",
    "print(capacity1, capacity2, capacity3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions_sample_test'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range_pred = df['predictions_sample'].max() - df['predictions_sample'].min() \n",
    "\n",
    "range_pred = 1000000 / 10000\n",
    "print(\"error_pctg = \", 1/range_pred)\n",
    "#print(range_pred / np.power(2,6))\n",
    "#print(range_pred / np.power(2,3))\n",
    "print(\"range_pred={}, log2(range_pred)={}\".format(range_pred, math.log2(range_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter infinity values\n",
    "print(df.shape)\n",
    "\n",
    "df = df[df['SNR_predictions'] < 10000]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_errors_val = (df_validation['y_val_actual'] - df_validation['y_val_pred'])\n",
    "#prediction_errors_val = prediction_errors_val[np.abs(prediction_errors_val) / df_validation['y_val_actual'] < 0.5]\n",
    "plt.hist(prediction_errors_val, bins = 200)\n",
    "plt.xlabel('Prediction error')\n",
    "plt.ylabel('Frequency')\n",
    "output_file = output_path + '/prediction_error_hist_validation_' + dataset_name + '.png'\n",
    "print(output_file)\n",
    "plt.savefig(output_file)\n",
    "\n",
    "\n",
    "dataset_prediction_error_level = np.var(prediction_errors_val)\n",
    "print(\"prediction_errors_val = \", dataset_prediction_error_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_errors = (df['actual_sample_test'] - df['predictions_sample_test'])\n",
    "prediction_errors = prediction_errors[np.abs(prediction_errors)/df['actual_sample_test'] < 0.5]\n",
    "plt.hist(prediction_errors, bins = 200)\n",
    "plt.xlabel('Prediction error')\n",
    "plt.ylabel('Frequency')\n",
    "output_file = output_path + '/prediction_error_hist_' + dataset_name + '.png'\n",
    "print(output_file)\n",
    "plt.savefig(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['SNR'] = df['target_energy_level'] / df['noise_level']\n",
    "\n",
    "# Calculate the SNR\n",
    "mean_noise = df[df['noise_level'] > 1.000].mean()\n",
    "df[df['noise_level'] == 1.000]['noise_level'] = mean_noise\n",
    "\n",
    "\n",
    "# local samples noise level SNR\n",
    "df['SNR'] = df['predictions_sample_test']**2 / df['noise_level']\n",
    "\n",
    "# Global dataset noise level SNR\n",
    "#df['SNR'] = df['predictions_sample_test']**2 / dataset_prediction_error_level \n",
    "\n",
    "SNR_hist = df['SNR'][df['SNR'] < 10000]\n",
    "SNR_hist.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_queries_in_subspace'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#df_filtered = df[df['SNR_predictions'] < 0.2*1e11]\n",
    "#df_filtered = df_filtered[df_filtered['SNR_predictions'] > 0.4*1e10]\n",
    "df_filtered = df[df['SNR'] < 100]\n",
    "#df_filtered = df_filtered[df_filtered['SNR_predictions'] > 0.4*1e10]\n",
    "\n",
    "ax.scatter(df_filtered['SNR'], df_filtered['losses_val'])\n",
    "#ax.title('Prediction losses [%] vs. SNR per prediction')\n",
    "plt.gca().update(dict(title='Prediction losses [%] vs. SNR per prediction [Low SNR]'))\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(\"SNR\")\n",
    "ax.set_ylabel(\"loss %\")\n",
    "\n",
    "print('Variance of losses =', df_filtered['losses_val'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#df_filtered = df[df['SNR_predictions'] > 1e13]\n",
    "#df_filtered = df_filtered[df_filtered['SNR_predictions'] < 2e13]\n",
    "df_filtered = df[df['SNR'] > 1000]\n",
    "df_filtered = df_filtered[df_filtered['SNR'] < 10000]\n",
    "\n",
    "\n",
    "ax.scatter(df_filtered['SNR'], df_filtered['losses_val'])\n",
    "#ax.title('Prediction losses [%] vs. SNR per prediction')\n",
    "plt.gca().update(dict(title='Prediction losses [%] vs. SNR per prediction [Medium SNR]', xlabel='x', ylabel='y'))\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(\"SNR\")\n",
    "ax.set_ylabel(\"loss %\")\n",
    "print('Variance of losses =', df_filtered['losses_val'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#df_filtered = df[df['SNR_predictions'] > 2e13]\n",
    "#df_filtered = df_filtered[df_filtered['SNR_predictions'] < 4e13]\n",
    "df_filtered = df[df['SNR'] > 10000]\n",
    "df_filtered = df_filtered[df_filtered['SNR'] < 1000000]\n",
    "\n",
    "ax.scatter(df_filtered['SNR'], df_filtered['losses_val'])\n",
    "#ax.title('Prediction losses [%] vs. SNR per prediction')\n",
    "plt.gca().update(dict(title='Prediction losses [%] vs. SNR per prediction [High SNR]', xlabel='x', ylabel='y'))\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(\"SNR\")\n",
    "ax.set_ylabel(\"loss %\")\n",
    "print('Variance of losses =', df_filtered['losses_val'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "df_filtered = df[df['SNR'] > 1e6]\n",
    "df_filtered = df_filtered[df_filtered['SNR'] < 1e13]\n",
    "\n",
    "ax.scatter(df_filtered['SNR'], df_filtered['losses_val'])\n",
    "#ax.title('Prediction losses [%] vs. SNR per prediction')\n",
    "plt.gca().update(dict(title='Prediction losses [%] vs. SNR per prediction', xlabel='x', ylabel='y'))\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(\"SNR\")\n",
    "ax.set_ylabel(\"loss %\")\n",
    "print('Variance of losses =', df_filtered['losses_val'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['SNR'][df['SNR'] == 1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['SNR'][df['SNR'] != 1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#capacity = []\n",
    "#capacity3 = 0.5 * (math.log2(1 + 10000))\n",
    "# [-A......|......+A] == 1 bit\n",
    "# \n",
    "# A + A*j  ==> 2 bits\n",
    "# -A + A*j\n",
    "# A - A*j\n",
    "# -A - A*j\n",
    "\n",
    "# X_real_target ==> Hunch(X_real_target) ==> Hunch(X_features) = X_pred_target = X_real_target + n \n",
    "# ==> such that, n is AWGN(0, variance)\n",
    "# \n",
    "# Tx symbols bits <= shannon_capacity = 1.5 bits / channel usage \n",
    "# \n",
    "# global query space ==> target_real is always in range = [target_real_min, target_real_max]\n",
    "# bits_in_range = log2((target_real_max - target_real_min) / 2^10)\n",
    "#               = [0, 1, 2, .., K-1] ==> [0, 2^10, 2*2^10, ..., (K-1)*2^10]\n",
    "# \n",
    "df['shannon_capacity'] = 0.5 * (np.log2(1 +  df['SNR']))\n",
    "df['shannon_capacity'].hist(bins=20)\n",
    "\n",
    "#ax.scatter(df_filtered['SNR'], df_filtered['losses'])\n",
    "##ax.title('Prediction losses [%] vs. SNR per prediction')\n",
    "#plt.gca().update(dict(title='Prediction losses [%] vs. Capacity (high bound of rate)', xlabel='x', ylabel='y'))\n",
    "#ax.legend(fontsize=8)\n",
    "#ax.set_xlabel(\"SNR\")\n",
    "#ax.set_ylabel(\"loss %\")\n",
    "#print('Variance of losses =', df_filtered['losses'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['actual_pred'] =  df['predictions_sample'] / (df['losses']/100)\n",
    "#df['actual_pred'].hist(bins=10)\n",
    "\n",
    "#df[df['shannon_capacity'] < 2]['losses'].mean()\n",
    "#df[df['shannon_capacity'] > 2]['losses'].mean()\n",
    "\n",
    "min_val = df_filtered['predictions_sample_test'].min()\n",
    "max_val = df_filtered['predictions_sample_test'].max()\n",
    "bits = np.log2(max_val - min_val)\n",
    "\n",
    "loss_variance_as_function_of_capacity = []\n",
    "\n",
    "# Build capacities list we want to run over\n",
    "capacities = []\n",
    "step_size = 2\n",
    "initial_value = 0 * step_size\n",
    "last_capacity = 7\n",
    "#last_capacity = 20\n",
    "for l in range(initial_value, last_capacity * step_size):\n",
    "    capacities.append(l / step_size)\n",
    "    \n",
    "count_samples = []\n",
    "loss_means = []\n",
    "max_errors = []\n",
    "prediction_losses_test = []\n",
    "prediction_losses_mean_test = []\n",
    "pctg_samples_larger_capacity_test = []\n",
    "mean_num_queries_in_subspace_on_threshold_capacity = []\n",
    "loss_variance_testset_on_threshold_capacity = []\n",
    "\n",
    "mean_num_bits_required_testset_on_threshold_capacity = []\n",
    "\n",
    "confidence_interval_per_capacity_low_bound = []\n",
    "confidence_interval_per_capacity_high_bound = []\n",
    "confidence_interval_per_capacity_length = []\n",
    "confidence_interval_per_capacity_coverage_ratio = []\n",
    "\n",
    "queries_per_capacity_total = []\n",
    "\n",
    "mean_target_actual_value_on_capacity = []\n",
    "median_target_actual_value_on_capacity = []\n",
    "percentile_0_25_target_actual_value_on_capacity = []\n",
    "percentile_0_50_target_actual_value_on_capacity = []\n",
    "percentile_0_75_target_actual_value_on_capacity = []\n",
    "\n",
    "# Calculate Hunch PI\n",
    "Hunch_PI = np.sqrt(sum((df['actual_sample_test'] - df['predictions_sample_test'])**2) / len(df['predictions_sample_test']))\n",
    "print(\"Hunch_PI = \", Hunch_PI)\n",
    "\n",
    "total_hunch_pi_coverage_queries = 0\n",
    "total_queries = 0\n",
    "\n",
    "N = df['shannon_capacity'].count()\n",
    "classification_df = pd.DataFrame()\n",
    "for capacity in capacities:\n",
    "    cond1 = (df['shannon_capacity'] >= capacity)\n",
    "    cond2 = (df['shannon_capacity'] < (capacity + (1 / step_size)))\n",
    "    df_filtered = df[cond1 & cond2]\n",
    "    print(capacity)\n",
    "    df_filtered['shannon_capacity'].hist()\n",
    "    \n",
    "    #df_filtered = df[(df['shannon_capacity'] >= capacity) & (df['shannon_capacity'] < (capacity+1))]\n",
    "    df_filtered_all_larger = df_filtered\n",
    "    #df_filtered_all_larger = df[(df['shannon_capacity'] >= capacity)]\n",
    "\n",
    "    # Important: Prevent nans !!!!\n",
    "    df_filtered = df_filtered[df_filtered['losses_val'] < infinity_loss_definition]\n",
    "    df_filtered = df_filtered[df_filtered['losses_val'] > -infinity_loss_definition]\n",
    "\n",
    "    df_filtered['minimum_capacity'] = capacity\n",
    "    classification_df = pd.concat([classification_df, df_filtered], axis=0)\n",
    "\n",
    "    loss_variance = np.var(df_filtered['losses_val'])\n",
    "    loss_mean = np.mean(df_filtered['losses_val'])\n",
    "\n",
    "    K = df_filtered_all_larger['shannon_capacity'].count()\n",
    "    \n",
    "    # Do we still have enough samples for statistics?\n",
    "    if (K/N) < 0.01:\n",
    "        continue\n",
    "\n",
    "    pctg_samples_larger_capacity_test.append(K/N)\n",
    "\n",
    "    loss_variance_as_function_of_capacity.append(loss_variance)\n",
    "    count_samples.append(df_filtered['shannon_capacity'].count())\n",
    "    loss_means.append(loss_mean)\n",
    "\n",
    "    a = df_filtered_all_larger['actual_sample_test']\n",
    "    b = df_filtered_all_larger['predictions_sample_test']\n",
    "\n",
    "    losses_on_capacity = 100 * ((a - b) / a)\n",
    "    \n",
    "    mean_target_actual_value_on_capacity.append(df_filtered_all_larger['actual_sample_test'].mean())\n",
    "    \n",
    "    # TODO: Add Median, 25%, 50% and 75%\n",
    "    median_target_actual_value_on_capacity.append(df_filtered_all_larger['actual_sample_test'].median())\n",
    "\n",
    "    percentile_0_25_target_actual_value_on_capacity.append(np.percentile(df_filtered_all_larger['actual_sample_test'], 25))\n",
    "    percentile_0_50_target_actual_value_on_capacity.append(np.percentile(df_filtered_all_larger['actual_sample_test'], 50))\n",
    "    percentile_0_75_target_actual_value_on_capacity.append(np.percentile(df_filtered_all_larger['actual_sample_test'], 75))\n",
    "    \n",
    "    # number of samples used in validation space\n",
    "    mean_num_queries_in_subspace_on_threshold_capacity.append(df_filtered_all_larger['num_queries_in_subspace'].mean())\n",
    "\n",
    "    # number of bits required to represent {max - min}\n",
    "    max_in_queries = df_filtered_all_larger['max_value_in_query_subspace_val']\n",
    "    min_in_queries = df_filtered_all_larger['min_value_in_query_subspace_val']\n",
    "    mean_num_bits = np.log2(abs(max_in_queries - min_in_queries).mean())\n",
    "    mean_num_bits_required_testset_on_threshold_capacity.append(mean_num_bits)\n",
    "    \n",
    "    # Calculate CI (Confidence Interval) based on T student distribution - For each Shannon Capacity:\n",
    "    #mean, sigma = np.mean(losses_on_capacity), np.std(losses_on_capacity)\n",
    "    predictions_with_capacity_larger = df_filtered_all_larger['predictions_sample_test'].copy()\n",
    "    sigma = np.std(predictions_with_capacity_larger) / stats_norm_interval_division_factor_const\n",
    "    mean = np.mean(predictions_with_capacity_larger)\n",
    "    conf_int = stats.norm.interval(0.95, loc=mean, scale=sigma)\n",
    "    #print(\"Confidence Interval: \", conf_int)\n",
    "    low_bound = conf_int[0]\n",
    "    high_bound = conf_int[1]\n",
    "    confidence_interval_per_capacity_low_bound.append(low_bound)\n",
    "    confidence_interval_per_capacity_high_bound.append(high_bound)\n",
    " \n",
    "    # The total number of queries per capacity\n",
    "    queries_per_capacity_total.append(len(predictions_with_capacity_larger))\n",
    "\n",
    "    # Calculate the interval length mean average for each shannon capacity and plot in plot 6 \n",
    "    # and put a number in each shannon capacity\n",
    "    confidence_interval_per_capacity_length.append(high_bound - low_bound)\n",
    "\n",
    "    # Coverage ==> Number of times where a target falls between the prediction interval for each \n",
    "    #              Shannon Capacity divided by the number of queries for each shannon capacity\n",
    "    #\n",
    "    coverage_samples = predictions_with_capacity_larger[predictions_with_capacity_larger < high_bound]\n",
    "    coverage_samples = predictions_with_capacity_larger[predictions_with_capacity_larger >= low_bound]\n",
    "    coverage = coverage_samples.count()\n",
    "    confidence_interval_coverage_ratio = coverage / predictions_with_capacity_larger.count()\n",
    "    confidence_interval_per_capacity_coverage_ratio.append(confidence_interval_coverage_ratio)\n",
    "    \n",
    "    # Add total samples that fall into the HunchPI\n",
    "    total_hunch_pi_coverage_queries += coverage\n",
    "    total_queries += predictions_with_capacity_larger.count()   \n",
    "\n",
    "    predictions_with_capacity_larger = df_filtered_all_larger['predictions_sample_test'].copy()\n",
    "    #coverage_samples = predictions_with_capacity_larger[predictions_with_capacity_larger < v2]\n",
    "    #coverage_samples = predictions_with_capacity_larger[predictions_with_capacity_larger >= v1]\n",
    "    #coverage = coverage_samples.count()\n",
    "    #confidence_interval_coverage_ratio = coverage / predictions_with_capacity_larger.count()\n",
    "   \n",
    "    # important: prevent nans\n",
    "    losses_on_capacity = losses_on_capacity[losses_on_capacity < infinity_loss_definition]\n",
    "    losses_on_capacity = losses_on_capacity[losses_on_capacity > -infinity_loss_definition]\n",
    "    #print(\"losses_on_capacity = \", losses_on_capacity)\n",
    "    \n",
    "    loss_test = losses_on_capacity.std()\n",
    "    loss_test_mean = losses_on_capacity.mean()\n",
    "\n",
    "    # New variance when cutting off the capacities lower (i.e. they are read directly from DB with regular query)\n",
    "    # prevent divide by 0\n",
    "    if (N == 0):\n",
    "        N = 1\n",
    "    \n",
    "    loss_variance_testset_on_threshold_capacity.append(loss_test * ((K/N)**2))\n",
    "\n",
    "    prediction_losses_test.append(loss_test)\n",
    "    prediction_losses_mean_test.append(loss_test_mean)\n",
    "    \n",
    "    #min_val = df_filtered['predictions_sample_val'].min()\n",
    "    #max_val = df_filtered['predictions_sample_val'].max()\n",
    "    #bits = np.log2(max_val - min_val)\n",
    "    max_error = (2**bits) / (2**capacity)\n",
    "    max_errors.append(max_error)\n",
    "    #print(\"capacity={}, min_val={}, max_val={}, bits={} max_error={}, loss_test={}, loss_test_mean={}\".\\\n",
    "    #      format(capacity, min_val, max_val, bits, max_error,loss_test, loss_test_mean))\n",
    "\n",
    "if 0:\n",
    "    print(\"capacities = \", capacities)\n",
    "    print(\"loss variance for each capacity in [capacities[i],capacities[i]+1] = \", loss_variance_as_function_of_capacity)\n",
    "    print(\"loss mean for each capacity in [capacities[i],capacities[i]+1] = \", loss_means)\n",
    "\n",
    "    print(\"prediction_losses_test = \", prediction_losses_test)\n",
    "    print(\"count_samples = \", count_samples)\n",
    "\n",
    "    print(\"max_errors > loss_variance_as_function_of_capacity ? ==> \",\\\n",
    "          max_errors > loss_variance_as_function_of_capacity)\n",
    "\n",
    "    print(\"pctg_samples_larger_capacity_test = \", pctg_samples_larger_capacity_test)\n",
    "\n",
    "fig, ax = plt.subplots(10)\n",
    "#plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "plt.gcf().set_size_inches(20, 60, forward=True) \n",
    "\n",
    "# Cut capacities\n",
    "capacities = capacities[0:len(prediction_losses_test)]\n",
    "\n",
    "x_label_name_shannon_capacity = \"Minimum Shannon's capacity\"\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "# plot 1: prediction_loss (test data)\n",
    "#ax[plot_idx].scatter(capacities, loss_variance_as_function_of_capacity)\n",
    "ax[plot_idx].plot(capacities, prediction_losses_test)\n",
    "ax[plot_idx].legend(['prediction_loss (test data)'])\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"Loss Variance\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "plot_idx += 1\n",
    "\n",
    "# plot 2:% samples with larger capacity\n",
    "ax[plot_idx].plot(capacities, pctg_samples_larger_capacity_test)\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].legend()\n",
    "ax[plot_idx].legend(['% samples with larger capacity'])\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"Percentage of samples in test\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "\n",
    "plot_idx += 1\n",
    "\n",
    "# plot 3: loss mean with larger capacity\n",
    "ax[plot_idx].plot(capacities, prediction_losses_mean_test)\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].legend()\n",
    "ax[plot_idx].legend(['loss mean with larger capacity'])\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"Prediction loss mean (test data)\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "plot_idx += 1\n",
    "\n",
    "# plot 4: loss mean with larger capacity (test data + DB query)\n",
    "ax[plot_idx].plot(capacities, loss_variance_testset_on_threshold_capacity)\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].legend()\n",
    "ax[plot_idx].legend(['loss mean with larger capacity (test data + DB query)'])\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"Prediction loss variance after applying (N/K)^2 (test data)\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "plot_idx += 1\n",
    "\n",
    "# plot 5: Mean of number of queries \n",
    "ax[plot_idx].plot(capacities, mean_num_queries_in_subspace_on_threshold_capacity)\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].legend()\n",
    "ax[plot_idx].legend(['Mean of number of queries in the same subspace in validation data'])\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"Mean of num_queries_in_subspace in validation data\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "\n",
    "plot_idx += 1\n",
    "\n",
    "# plot 5: Mean of log2{max-min}\n",
    "ax[plot_idx].plot(capacities, mean_num_bits_required_testset_on_threshold_capacity)\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].legend()\n",
    "ax[plot_idx].legend(['Mean of log2{max-min} in validation data with larger capacity'])\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"Mean of of log2{max-min} in validation data\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "plot_idx += 1\n",
    "\n",
    "# plot 6: CI (Confidence Interval)\n",
    "confidence_interval_per_capacity_length_text = 'CI low bound vs. high bound length, division_factor=' +\\\n",
    "                                               str(stats_norm_interval_division_factor_const)\n",
    "#print(confidence_interval_per_capacity_low_bound)\n",
    "ax[plot_idx].plot(capacities, confidence_interval_per_capacity_low_bound)\n",
    "ax[plot_idx].plot(capacities, confidence_interval_per_capacity_high_bound)\n",
    "ax[plot_idx].plot(capacities, mean_target_actual_value_on_capacity)\n",
    "ax[plot_idx].plot(capacities, confidence_interval_per_capacity_length)\n",
    "\n",
    "ax[plot_idx].plot(capacities, median_target_actual_value_on_capacity)\n",
    "ax[plot_idx].plot(capacities, percentile_0_25_target_actual_value_on_capacity)\n",
    "ax[plot_idx].plot(capacities, percentile_0_50_target_actual_value_on_capacity)\n",
    "ax[plot_idx].plot(capacities, percentile_0_75_target_actual_value_on_capacity)\n",
    "\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].legend()\n",
    "ax[plot_idx].legend(['CI in validation data with larger capacity LOW bound', 'CI in validation data with larger capacity HIGH bound',\n",
    "                     'Mean of actual target value >= capacity', confidence_interval_per_capacity_length_text,\n",
    "                     'Median of actual target value >= capacity',\n",
    "                     '0.25 percentile of actual target value >= capacity',\n",
    "                     '0.50 percentile of actual target value >= capacity',\n",
    "                     '0.75 percentile of actual target value >= capacity'])\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"CI in validation data\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "\n",
    "plot_idx += 1\n",
    "\n",
    "# Calculate the interval length mean average for each shannon capacity and plot in plot 6 and put a number in each shannon capacity\n",
    "\n",
    "# plot 7: CI (Confidence Interval coverage ratio)\n",
    "#print(confidence_interval_per_capacity_low_bound)\n",
    "ax[plot_idx].plot(capacities, confidence_interval_per_capacity_coverage_ratio)\n",
    "ax[plot_idx].legend(fontsize=8)\n",
    "ax[plot_idx].legend()\n",
    "ax[plot_idx].legend(['confidence interval coverage ratio'])\n",
    "ax[plot_idx].set_xlabel(x_label_name_shannon_capacity)\n",
    "ax[plot_idx].set_ylabel(\"CI in validation data\")\n",
    "ax[plot_idx].set_title(dataset_name)\n",
    "\n",
    "plot_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to results CSV\n",
    "df_snr_results['capacities'] = capacities\n",
    "df_snr_results['prediction_losses_test'] = prediction_losses_test\n",
    "df_snr_results['pctg_samples_larger_capacity_test'] = pctg_samples_larger_capacity_test\n",
    "df_snr_results['prediction_losses_mean_test'] = prediction_losses_mean_test\n",
    "df_snr_results['loss_variance_testset_on_threshold_capacity'] = loss_variance_testset_on_threshold_capacity\n",
    "df_snr_results['mean_num_queries_in_subspace_on_threshold_capacity'] = mean_num_queries_in_subspace_on_threshold_capacity\n",
    "df_snr_results['mean_num_bits_required_testset_on_threshold_capacity'] = mean_num_bits_required_testset_on_threshold_capacity\n",
    "df_snr_results['confidence_interval_per_capacity_coverage_ratio'] = confidence_interval_per_capacity_coverage_ratio\n",
    "df_snr_results['confidence_interval_per_capacity_low_bound'] = confidence_interval_per_capacity_low_bound\n",
    "df_snr_results['confidence_interval_per_capacity_high_bound'] = confidence_interval_per_capacity_high_bound\n",
    "df_snr_results['mean_target_actual_value_on_capacity'] = mean_target_actual_value_on_capacity\n",
    "df_snr_results['confidence_interval_per_capacity_length'] = confidence_interval_per_capacity_length\n",
    "df_snr_results['median_target_actual_value_on_capacity'] = median_target_actual_value_on_capacity\n",
    "df_snr_results['percentile_0_25_target_actual_value_on_capacity'] = percentile_0_25_target_actual_value_on_capacity\n",
    "df_snr_results['percentile_0_50_target_actual_value_on_capacity'] = percentile_0_50_target_actual_value_on_capacity\n",
    "df_snr_results['percentile_0_75_target_actual_value_on_capacity'] = percentile_0_75_target_actual_value_on_capacity\n",
    "df_snr_results['confidence_interval_per_capacity_coverage_ratio'] = confidence_interval_per_capacity_coverage_ratio\n",
    "df_snr_results['queries_per_capacity_total'] = queries_per_capacity_total\n",
    "\n",
    "df_snr_results.to_csv(csv_vectors_output_filename, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "if 0:\n",
    "    columns_names_y = ['prediction_losses_test',\n",
    "           'pctg_samples_larger_capacity_test', 'prediction_losses_mean_test',\n",
    "           'loss_variance_testset_on_threshold_capacity',\n",
    "           'mean_num_queries_in_subspace_on_threshold_capacity',\n",
    "           'mean_num_bits_required_testset_on_threshold_capacity',\n",
    "           'confidence_interval_per_capacity_coverage_ratio',\n",
    "           'confidence_interval_per_capacity_low_bound',\n",
    "           'confidence_interval_per_capacity_high_bound',\n",
    "           'mean_target_actual_value_on_capacity',\n",
    "           'confidence_interval_per_capacity_length',\n",
    "           'median_target_actual_value_on_capacity',\n",
    "           'percentile_0_25_target_actual_value_on_capacity',\n",
    "           'percentile_0_50_target_actual_value_on_capacity',\n",
    "           'percentile_0_75_target_actual_value_on_capacity']\n",
    "else:\n",
    "    #,\n",
    "    columns_names_y = ['prediction_losses_mean_test']\n",
    "                      #'pctg_samples_larger_capacity_test'] #['prediction_losses_test',       \n",
    "\n",
    "\n",
    "    \n",
    "# reshape the dataframe \n",
    "#dfm = df_snr_results.melt(id_vars=columns_names_y)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# loading dataset\n",
    "#data = sns.load_dataset(\"tips\")\n",
    "  \n",
    "# draw pointplot\n",
    "\n",
    "plot_results = df_snr_results.copy()\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "\n",
    "# Normalize\n",
    "if 0:\n",
    "    max_val = 0\n",
    "    for col in columns_names_y:\n",
    "        if (max_val < max(plot_results[col])):\n",
    "            max_val = max(plot_results[col])\n",
    "else:\n",
    "    max_val = 1\n",
    "\n",
    "for col in columns_names_y:\n",
    "    plot_results[col] = plot_results[col] / max_val\n",
    "\n",
    "df = plot_results.melt(\"capacities\", columns_names_y)\n",
    "\n",
    "ax = sns.pointplot(data=df, x='capacities', y='value', hue=\"variable\", ci='sd', dodge=0.25)\n",
    "\n",
    "#for col in columns_names_y:\n",
    "#    plot_results[col] = plot_results[col] / max(plot_results[col])\n",
    "#    sns.pointplot(x='capacities', y=col, data=plot_results, ci='sd', dodge=0.25)\n",
    "#c=sns.pointplot(data = df, x='Date', y='TotalConfirmed', color=\"b\", \n",
    "#                label='Total Confirmed')\n",
    "#d=sns.pointplot(data = df, x='Date', y='TotalDeath', color=\"r\", \n",
    "#                label='Total Death')\n",
    "#r=sns.pointplot(data = df, x='Date', y='TotalRecovered', color=\"g\", \n",
    "#                label='Total Recovered')\n",
    "ax.set_title('Dataset_' + dataset_name_to_index(dataset_name), fontsize=22, y=1.015)\n",
    "ax.set_xlabel(\"Minimum Shannon's Capacity\", labelpad=16)\n",
    "#ax.set_ylabel('# of people', labelpad=16)\n",
    "#ax.set(yscale=\"log\")\n",
    "t=plt.xticks(rotation=45)\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "#p = sns.pointplot(data=dfm, x='capacities', hue='prediction_losses_test', y='prediction_losses_test',\n",
    "#                  ci='sd', dodge=0.25) #hue='g', ) #, y='value'\n",
    "#p.set_title('Error bars are standard deviation') \n",
    "#p.legend(title='g', bbox_to_anchor=(1.05, 1), loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "#plt.gcf().set_size_inches(20, 60, forward=True) \n",
    "\n",
    "# Cut capacities\n",
    "capacities = capacities[0:len(prediction_losses_test)]\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "# plot 1: prediction_loss (test data)\n",
    "#ax[plot_idx].scatter(capacities, loss_variance_as_function_of_capacity)\n",
    "prediction_losses_test_normalized = prediction_losses_test / np.max(prediction_losses_test)\n",
    "ax.plot(capacities, prediction_losses_test)\n",
    "ax.legend(['prediction_loss (test data)'])\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"Loss Variance (Normalized)\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_loss_test_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "#plt.gcf().set_size_inches(20, 60, forward=True) \n",
    "plot_idx = 0\n",
    "\n",
    "# plot 2:% samples with larger capacity\n",
    "ax.plot(capacities, pctg_samples_larger_capacity_test)\n",
    "ax.legend(fontsize=8)\n",
    "ax.legend()\n",
    "ax.legend(['% samples with larger capacity'])\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"Percentage of samples in test\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_pctg_samples_larger_capacity_test_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_idx = 0\n",
    "\n",
    "# plot 3: loss mean with larger capacity\n",
    "ax.plot(capacities, prediction_losses_mean_test)\n",
    "ax.legend(fontsize=8)\n",
    "ax.legend()\n",
    "ax.legend(['loss mean with larger capacity'])\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"Prediction loss mean (test data)\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_loss_mean_larger_capacity_test_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_idx = 0\n",
    "\n",
    "# plot 4: loss mean with larger capacity (test data + DB query)\n",
    "ax.plot(capacities, loss_variance_testset_on_threshold_capacity)\n",
    "ax.legend(fontsize=8)\n",
    "ax.legend()\n",
    "ax.legend(['loss mean with larger capacity (test data + DB query)'])\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"Prediction loss variance after applying (N/K)^2 (test data)\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_loss_mean_larger_capacity_test_data_with_db_query_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_idx = 0\n",
    "\n",
    "# plot 5: Mean of number of queries \n",
    "ax.plot(capacities, mean_num_queries_in_subspace_on_threshold_capacity)\n",
    "ax.legend(fontsize=8)\n",
    "ax.legend()\n",
    "ax.legend(['Mean num of queries in validation data in SNR'])\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"Mean num queries in validation for SNR\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_mean_num_queries_in_subspace_validation_data_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_idx = 0\n",
    "\n",
    "# plot 5: Mean of log2{max-min}\n",
    "ax.plot(capacities, mean_num_bits_required_testset_on_threshold_capacity)\n",
    "ax.legend(fontsize=8)\n",
    "ax.legend()\n",
    "ax.legend(['Mean log2{max-min} validation data (larger capacity)'])\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"Mean of of log2{max-min} in validation data\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_mean_log2_max_minus_min_validation_data_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_idx = 0\n",
    "\n",
    "plt.gcf().set_size_inches(10, 10, forward=True) \n",
    "\n",
    "# plot 6: CI (Confidence Interval)\n",
    "confidence_interval_per_capacity_length_text = 'CI low bound vs. high bound length, division_factor=' +\\\n",
    "                                               str(stats_norm_interval_division_factor_const)\n",
    "#print(confidence_interval_per_capacity_low_bound)\n",
    "ax.plot(capacities, confidence_interval_per_capacity_low_bound)\n",
    "ax.plot(capacities, confidence_interval_per_capacity_high_bound)\n",
    "ax.plot(capacities, mean_target_actual_value_on_capacity)\n",
    "ax.plot(capacities, confidence_interval_per_capacity_length)\n",
    "\n",
    "\n",
    "ax.plot(capacities, median_target_actual_value_on_capacity)\n",
    "ax.plot(capacities, percentile_0_25_target_actual_value_on_capacity)\n",
    "ax.plot(capacities, percentile_0_50_target_actual_value_on_capacity)\n",
    "ax.plot(capacities, percentile_0_75_target_actual_value_on_capacity)\n",
    "\n",
    "ax.legend(fontsize=8)\n",
    "ax.legend()\n",
    "ax.legend(['CI in validation data with larger capacity LOW bound', 'CI in validation data with larger capacity HIGH bound',\n",
    "                     'Mean of actual target value >= capacity', confidence_interval_per_capacity_length_text,\n",
    "                     'Median of actual target value >= capacity',\n",
    "                     '0.25 percentile of actual target value >= capacity',\n",
    "                     '0.50 percentile of actual target value >= capacity',\n",
    "                     '0.75 percentile of actual target value >= capacity'])\n",
    "\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"CI in validation data\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_confidence_interval_validation_data_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the interval length mean average for each shannon capacity and plot in plot 6 and put a number in each shannon capacity\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_idx = 0\n",
    "\n",
    "# plot 7: CI (Confidence Interval coverage ratio)\n",
    "#print(confidence_interval_per_capacity_low_bound)\n",
    "ax.plot(capacities, confidence_interval_per_capacity_coverage_ratio)\n",
    "ax.legend(fontsize=8)\n",
    "ax.legend()\n",
    "ax.legend(['CI coverage ratio (by Shannon''s capacity)'])\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"CI in validation data\")\n",
    "ax.set_title(dataset_name_to_index(dataset_name))\n",
    "\n",
    "filename_out = output_path + \"/prediction_confidence_interval_coverage_ratio_validation_data_\" +\\\n",
    "        dataset_name_to_index(dataset_name) + \".png\"\n",
    "plt.savefig(filename_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(capacities, loss_means)\n",
    "\n",
    "plt.gca().update(dict(title='Loss Mean vs. Shannon Capacity', xlabel='x', ylabel='y'))\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(x_label_name_shannon_capacity)\n",
    "ax.set_ylabel(\"Loss Mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df.to_csv(csv_vectors_output_filename, index = False)\n",
    "\n",
    "classification_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MPIW_calculate(low_bound_per_capacity, high_bound_per_capacity, queries_per_capacity):\n",
    "    mpiw = 0\n",
    "    total_queries = 0\n",
    "    for idx in range(0, len(queries_per_capacity)):\n",
    "        width = high_bound_per_capacity[idx] - low_bound_per_capacity[idx]\n",
    "        mpiw += (width * queries_per_capacity[idx])\n",
    "        total_queries += queries_per_capacity[idx]\n",
    "\n",
    "    mpiw /= total_queries\n",
    "    \n",
    "    return mpiw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coverage for each PI [PICP]\n",
    "HunchPI_coverage = total_hunch_pi_coverage_queries / total_queries\n",
    "\n",
    "# Calculate the MPIW\n",
    "HunchPI_MPIW = MPIW_calculate(df_snr_results['confidence_interval_per_capacity_low_bound'],\n",
    "                              df_snr_results['confidence_interval_per_capacity_high_bound'],\n",
    "                              df_snr_results['queries_per_capacity_total'])\n",
    "\n",
    "print(\"Dataset name = \",  dataset_name)\n",
    "print(\"Dataset index = \",  dataset_name_to_index(dataset_name))\n",
    "print(\"total_hunch_pi_coverage_queries = \", total_hunch_pi_coverage_queries)\n",
    "print(\"total_queries = \", total_queries)\n",
    "print(\"HunchPI_coverage [PICP] = \", HunchPI_coverage)\n",
    "print(\"HunchPI [log10(MPIW)] = \", np.log10(HunchPI_MPIW))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
